{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Severity of road traffic accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Yifan Wu\" -u -d -t -v -p numpy,pandas,matplotlib,scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "- [Github link](https://github.com/Van-Wu1/0006.git)\n",
    "\n",
    "- Number of words: ***\n",
    "\n",
    "- Runtime: *** hours (*Memory 32 GB, CPU AMD Ryzen 7 5800H with Radeon Graphics CPU @3.20GHz*)\n",
    "\n",
    "- Coding environment: Coding environment: VS Code with Jupyter plugin (local), not SDS Docker\n",
    "\n",
    "- License: this notebook is made available under the [Creative Commons Attribution license](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "- Additional library *[libraries not included in SDS Docker or not used in this module]*:\n",
    "    - **watermark**: A Jupyter Notebook extension for printing timestamps, version numbers, and hardware information.(used to print Python and package versions for reproducibility.)\n",
    "    - **osmnx**: For downloading and analyzing OpenStreetMap road network data.\n",
    "    - **networkx**: For calculating road network metrics such as betweenness and degree centrality.\n",
    "    - **geopandas**: For spatial data handling, including reading GeoJSON borough boundaries.\n",
    "    - **shap**: For model interpretability using SHAP value analysis.\n",
    "    - **xgboost**: For gradient boosting machine learning classification.\n",
    "    - **tqdm**: For displaying progress bars during borough-level computations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "\n",
    "1. [Research questions](#Research-questions)\n",
    "\n",
    "1. [Data](#Data)\n",
    "\n",
    "1. [Methodology](#Methodology)\n",
    "\n",
    "1. [Results and discussion](#Results-and-discussion)\n",
    "\n",
    "1. [Conclusion](#Conclusion)\n",
    "\n",
    "1. [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Road traffic accidents (RTAs) represent a significant public health and urban governance issue globally. In the UK, despite advancements in vehicle technology and traffic regulation, thousands of individuals are injured or killed on the roads annually. Predicting the severity of these accidents is crucial for targeted policy interventions and infrastructure planning. Accident severity is influenced by a range of contextual factors including weather, road geometry, traffic volume, time of day, and infrastructure design (Abdel-Aty & Haleem, 2011). As cities move towards data-driven governance, the application of machine learning methods to road safety research has become increasingly common, offering more accurate and interpretable approaches to understanding complex risk patterns (Ahmed et al., 2023).\n",
    "\n",
    "Recent literature has demonstrated the effectiveness of supervised learning algorithms such as logistic regression, random forests, and XGBoost in predicting accident severity using structured datasets (Ahmed et al., 2023). These models are particularly suitable for capturing non-linear interactions and heterogeneous effects among multiple explanatory variables. Moreover, explainable AI techniques such as SHAP (SHapley Additive exPlanations) have been widely adopted to interpret complex models and understand feature importance, which aids in translating statistical findings into actionable insights for policymakers.\n",
    "\n",
    "This study leverages the UK Department for Transport’s Road Safety Data (2015–2019), which documents detailed information on individual accident cases, including temporal, spatial, environmental, and infrastructural attributes. By integrating network-based features such as road betweenness centrality extracted via OpenStreetMap, this project attempts to bridge the gap between spatial network analysis and predictive modelling of accident severity. The objective is twofold: to evaluate the predictive performance of commonly used machine learning models on accident severity classification, and to examine the relative contribution of different spatial and contextual factors to the outcome.\n",
    "\n",
    "The period from 2015 to 2019 was deliberately chosen to ensure data stability and validity. This timeframe avoids the confounding effects of the COVID-19 pandemic (2020–2021), which significantly disrupted travel behaviour, enforcement levels, and urban mobility patterns across the UK (DfT, 2021). It also precedes Phase 2 of London's major road transformation programme, including the expansion of Low Traffic Neighbourhoods (LTNs) and segregated cycling infrastructure, which introduced substantial structural changes to the transport system from 2020 onwards (TfL, 2024). In contrast, the preceding years (2010–2014) marked a foundational policy phase, characterised by the implementation of new traffic enforcement measures such as fixed penalty notices for careless driving and increased fines for common violations (DfT, 2013). By focusing on the relatively stable and mature period between 2015 and 2019, this study ensures greater internal consistency and enables clearer interpretation of accident severity patterns, isolated from exogenous policy or behavioural shocks.\n",
    "\n",
    "In contrast to most existing studies that primarily rely on tabular attributes such as time, weather, and road conditions, this project introduces spatial network metrics—specifically, betweenness and degree centrality—extracted from OpenStreetMap. This integration of spatial topology enhances the model's capacity to capture urban structural influences on accident severity, offering a novel bridge between road network analysis and predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research questions\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can supervised machine learning models accurately predict the severity of road traffic accidents in London using spatial, temporal, and environmental features?\n",
    "\n",
    "This project explores whether supervised machine learning models can accurately predict the severity of road traffic accidents in London based on spatial, temporal, and environmental features. Specifically, it evaluates the contribution of variables such as time of day, weather conditions, and road network centrality. The study compares the performance of Logistic Regression, Random Forest, and XGBoost classifiers, and employs SHAP (SHapley Additive exPlanations) to interpret model outputs and quantify feature importance across different severity levels (fatal, serious, slight)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable                                 | Type         | Description                                                                                  | Notes                                 |\n",
    "|------------------------------------------|--------------|----------------------------------------------------------------------------------------------|----------------------------------------|\n",
    "| accident_severity                        | Categorical  | Severity level of the accident (1 = Fatal, 2 = Serious, 3 = Slight)                          | Target variable                        |\n",
    "| speed_limit                              | Numeric      | Speed limit of the road segment (in mph)                                                     | -                                      |\n",
    "| accident_year                            | Numeric      | Year of the accident (2015–2019)                                                             | Used for train-test split              |\n",
    "| mean_betweenness                         | Numeric      | Mean betweenness centrality of nearby road segments                                          | Spatial network feature                |\n",
    "| max_betweenness                          | Numeric      | Maximum betweenness centrality of nearby road segments                                       | Key spatial variable                   |\n",
    "| mean_degree                              | Numeric      | Mean degree centrality of road network                                                       | -                                      |\n",
    "| max_degree                               | Numeric      | Maximum degree centrality                                                                    | -                                      |\n",
    "| edge_count                               | Numeric      | Number of road segments (edges) in the local road network                                    | Spatial indicator of network density   |\n",
    "| day_of_week_*                            | Categorical  | One-hot encoded day of week (Monday–Saturday, Sunday as baseline)                           | One-hot encoded                        |\n",
    "| road_type_*                              | Categorical  | One-hot encoded road type categories                                                         | One-hot encoded                        |\n",
    "| light_conditions_*                       | Categorical  | One-hot encoded lighting conditions (e.g., daylight, darkness with/without lighting)         | One-hot encoded                        |\n",
    "| weather_conditions_*                     | Categorical  | One-hot encoded weather conditions (e.g., fine, rain, fog)                                   | One-hot encoded                        |\n",
    "| road_surface_conditions_*                | Categorical  | One-hot encoded surface conditions (e.g., dry, wet)                                          | One-hot encoded                        |\n",
    "| junction_control_*                       | Categorical  | One-hot encoded control types at junctions                                                   | One-hot encoded                        |\n",
    "| junction_detail_*                        | Categorical  | One-hot encoded structural junction types                                                    | One-hot encoded                        |\n",
    "| pedestrian_crossing_human_control_*      | Categorical  | One-hot encoded presence of human-controlled crossings                                       | One-hot encoded                        |\n",
    "| pedestrian_crossing_physical_facilities_*| Categorical  | One-hot encoded presence of physical pedestrian facilities                                   | One-hot encoded                        |\n",
    "| special_conditions_at_site_*             | Categorical  | One-hot encoded site-specific conditions (e.g., roadworks)                                   | One-hot encoded                        |\n",
    "| first_road_class_*                       | Categorical  | One-hot encoded classification of the primary road                                           | One-hot encoded                        |\n",
    "| second_road_class_*                      | Categorical  | One-hot encoded classification of the secondary road                                         | One-hot encoded                        |\n",
    "| trunk_road_flag_*                        | Categorical  | One-hot encoded trunk road indicator                                                         | One-hot encoded                        |\n",
    "| urban_or_rural_area_*                    | Categorical  | One-hot encoded urban/rural area classification                                              | One-hot encoded                        |\n",
    "| time_hour                                | Numeric      | Hour of the accident (e.g., 13:55 → 13)                                                      | Derived feature                        |\n",
    "| betweenness_level_encoded                | Ordinal      | Quartile level of mean_betweenness (0 = Low, 3 = High)                                       | For logistic regression compatibility  |\n",
    "| ......  | ......  | ......                                    |   |\n",
    "\n",
    "\n",
    "> *Note: `*_` denotes one-hot encoded categories split into multiple columns.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table provides code-level descriptions for categorical variables used in this study. Definitions are based on the official UK Department for Transport data guide: [data.gov.uk](https://www.data.gov.uk/dataset/cb7ae6f0-4be6-4935-9277-47e5ce24a11f).\n",
    "\n",
    "| Variable Prefix                         | Code | Meaning                                   |\n",
    "|----------------------------------------|------|-------------------------------------------|\n",
    "| day_of_week                            | 1    | Sunday                                    |\n",
    "|                                        | 2    | Monday                                    |\n",
    "|                                        | 3    | Tuesday                                   |\n",
    "|                                        | 4    | Wednesday                                 |\n",
    "|                                        | 5    | Thursday                                  |\n",
    "|                                        | 6    | Friday                                    |\n",
    "|                                        | 7    | Saturday                                  |\n",
    "| road_type                              | 1    | Roundabout                                |\n",
    "|                                        | 2    | One way street                            |\n",
    "|                                        | 3    | Dual carriageway                          |\n",
    "|                                        | 6    | Single carriageway                        |\n",
    "|                                        | 7    | Slip road                                  |\n",
    "|                                        | 9    | Unknown                                   |\n",
    "| light_conditions                       | 1    | Daylight                                   |\n",
    "|                                        | 4    | Darkness - lights lit                      |\n",
    "|                                        | 5    | Darkness - lights unlit                    |\n",
    "|                                        | 6    | Darkness - no lighting                     |\n",
    "|                                        | 7    | Darkness - lighting unknown                |\n",
    "| weather_conditions                     | 1    | Fine no high winds                         |\n",
    "|                                        | 2    | Raining no high winds                      |\n",
    "|                                        | 3    | Snowing no high winds                      |\n",
    "|                                        | 4    | Fine + high winds                          |\n",
    "|                                        | 5    | Raining + high winds                       |\n",
    "|                                        | 6    | Snowing + high winds                       |\n",
    "|                                        | 7    | Fog or mist                                |\n",
    "|                                        | 8    | Other                                      |\n",
    "|                                        | 9    | Unknown                                    |\n",
    "| road_surface_conditions                | 1    | Dry                                        |\n",
    "|                                        | 2    | Wet or damp                                |\n",
    "|                                        | 3    | Snow                                       |\n",
    "|                                        | 4    | Frost or ice                               |\n",
    "|                                        | 5    | Flood (surface water)                      |\n",
    "|                                        | 9    | Unknown                                    |\n",
    "| junction_control                       | 0    | None                                       |\n",
    "|                                        | 1    | Authorised person                          |\n",
    "|                                        | 2    | Auto traffic signal                        |\n",
    "|                                        | 3    | Stop sign                                  |\n",
    "|                                        | 4    | Give way or uncontrolled                   |\n",
    "|                                        | 9    | Unknown                                    |\n",
    "| pedestrian_crossing_human_control      | 0    | None                                       |\n",
    "|                                        | 1    | School crossing patrol                     |\n",
    "|                                        | 2    | Other human control                        |\n",
    "|                                        | 9    | Unknown                                    |\n",
    "| pedestrian_crossing_physical_facilities| 0    | None                                       |\n",
    "|                                        | 1    | Zebra crossing                             |\n",
    "|                                        | 4    | Pelican crossing                           |\n",
    "|                                        | 5    | Footbridge or subway                       |\n",
    "|                                        | 7    | Refuge                                     |\n",
    "|                                        | 8    | Unknown                                     |\n",
    "|                                        | 9    | Other                                      |\n",
    "| urban_or_rural_area                    | 1    | Not used                                   |\n",
    "|                                        | 2    | Urban                                      |\n",
    "|                                        | 3    | Rural                                      |\n",
    "| trunk_road_flag                        | 1    | Non-trunk road                             |\n",
    "|                                        | 2    | Trunk road                                 |\n",
    "| first_road_class / second_road_class   | 1    | Motorway                                   |\n",
    "|                                        | 2    | A(M) Road                                   |\n",
    "|                                        | 3    | A Road                                     |\n",
    "|                                        | 4    | B Road                                     |\n",
    "|                                        | 5    | C Road                                     |\n",
    "|                                        | 6    | Unclassified                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It would import the packages that would be used first. \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder\n",
    "input_folder = '../data/raw'\n",
    "output_folder = '../data/clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Road Data\n",
    "df = pd.read_csv('../data/raw/1979-latest-published-year.csv')\n",
    "df = df[df['accident_year'].isin([2015, 2016, 2017, 2018, 2019])]\n",
    "print(f\"The data volume from 2015 to 2019 is:{len(df)} \")\n",
    "\n",
    "# save\n",
    "df.to_csv(\"../data/raw/2015_2019.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    'accident_severity',\n",
    "    'number_of_vehicles',\n",
    "    'number_of_casualties',\n",
    "    'day_of_week',\n",
    "    'time',\n",
    "    'first_road_class',\n",
    "    'second_road_class',\n",
    "    'road_type',\n",
    "    'speed_limit',\n",
    "    'junction_detail',\n",
    "    'junction_control',\n",
    "    'pedestrian_crossing_human_control',\n",
    "    'pedestrian_crossing_physical_facilities',\n",
    "    'light_conditions',\n",
    "    'weather_conditions',\n",
    "    'road_surface_conditions',\n",
    "    'special_conditions_at_site',\n",
    "    'carriageway_hazards',\n",
    "    'urban_or_rural_area',\n",
    "    'did_police_officer_attend_scene_of_accident',\n",
    "    'trunk_road_flag',\n",
    "    'local_authority_ons_district',\n",
    "    'accident_year'\n",
    "]\n",
    "\n",
    "selected_columns = [col for col in columns_to_keep if col in df.columns]\n",
    "df_cleaned = df[selected_columns]\n",
    "\n",
    "# Check and handle the missing values\n",
    "missing_counts = df_cleaned.isnull().sum()\n",
    "total_missing = missing_counts.sum()\n",
    "\n",
    "if total_missing > 0:\n",
    "    print(f\"The number of missing values are {total_missing} :\")\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "    \n",
    "    # Discard the rows containing missing values\n",
    "    df_cleaned = df_cleaned.dropna()\n",
    "    print(f\"Missing values have been cleared, remaining {len(df_cleaned)} records.\")\n",
    "\n",
    "# Save the cleaned files\n",
    "df_cleaned.to_csv('../data/clean/1519_cleaned.csv', index=False)\n",
    "print(f\"Saved to: {output_folder}, total: {len(df_cleaned.columns)} columns, {len(df_cleaned)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Feature Engineering\n",
    "\n",
    "This step extracts borough-level road networks from OpenStreetMap and calculates betweenness and degree centrality to capture spatial structure in the transport network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoadCentrality\n",
    "path = \"../data/Borough_Boundaries.geojson\"\n",
    "boroughs = gpd.read_file(path)\n",
    "boroughs = boroughs[[\"name\", \"gss_code\", \"geometry\"]].rename(columns={\"name\": \"borough\"})\n",
    "\n",
    "ox.settings.log_console = False\n",
    "ox.settings.use_cache = True\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in tqdm(boroughs.iterrows(), total=len(boroughs), desc=\"Processing boroughs\"):\n",
    "    borough_name = row[\"borough\"]\n",
    "    gss_name = row[\"gss_code\"]   \n",
    "    geometry = row[\"geometry\"]\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing: {borough_name}\")\n",
    "        \n",
    "        G = ox.graph_from_polygon(geometry, network_type=\"drive\", simplify=True)\n",
    "\n",
    "        betweenness = nx.betweenness_centrality(G, weight=\"length\", k=100, seed=42)\n",
    "        degree = dict(G.degree())\n",
    "        nx.set_node_attributes(G, betweenness, \"betweenness\")\n",
    "        nx.set_node_attributes(G, degree, \"degree\")\n",
    "\n",
    "        edge_data = []\n",
    "        for u, v, key, data in G.edges(keys=True, data=True):\n",
    "            edge_data.append({\n",
    "                \"u\": u,\n",
    "                \"v\": v,\n",
    "                \"key\": key,\n",
    "                \"geometry\": data.get(\"geometry\", None),\n",
    "                \"betweenness\": (G.nodes[u][\"betweenness\"] + G.nodes[v][\"betweenness\"]) / 2,\n",
    "                \"degree\": (G.nodes[u][\"degree\"] + G.nodes[v][\"degree\"]) / 2\n",
    "            })\n",
    "        edges_df = gpd.GeoDataFrame(edge_data, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "        summary = {\n",
    "            \"borough\": borough_name,\n",
    "            \"gss_code\": gss_name,\n",
    "            \"mean_betweenness\": edges_df[\"betweenness\"].mean(),\n",
    "            \"max_betweenness\": edges_df[\"betweenness\"].max(),\n",
    "            \"mean_degree\": edges_df[\"degree\"].mean(),\n",
    "            \"max_degree\": edges_df[\"degree\"].max(),\n",
    "            \"edge_count\": len(edges_df)\n",
    "        }\n",
    "        results.append(summary)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {borough_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(\"../data/london_borough_road_centrality.csv\", index=False)\n",
    "print(\"All done! Results saved to 'london_borough_road_centrality.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "print(\"Sample of calculated borough-level centrality metrics:\")\n",
    "display(df_results.head())\n",
    "\n",
    "# Display descriptive statistical information\n",
    "print(\"\\nSummary statistics of centrality metrics across boroughs:\")\n",
    "display(df_results[['mean_betweenness', 'mean_degree']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Merge & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path\n",
    "accident_path = \"../data/clean/1519_cleaned.csv\"\n",
    "centrality_path = \"../data/london_borough_road_centrality.csv\"\n",
    "output_path = \"../data/final/2015_2019_with_centrality.csv\"\n",
    "\n",
    "df_accident = pd.read_csv(accident_path)\n",
    "df_centrality = pd.read_csv(centrality_path)\n",
    "\n",
    "# Merge the centrality data (encoded by region)\n",
    "df_merged = df_accident.merge(\n",
    "    df_centrality,\n",
    "    how=\"left\",\n",
    "    left_on=\"local_authority_ons_district\",\n",
    "    right_on=\"gss_code\"\n",
    ")\n",
    "\n",
    "# Delete the rows lacking centrality (non-London area)\n",
    "before_drop = len(df_merged)\n",
    "df_merged = df_merged.dropna(subset=[\"mean_betweenness\"])\n",
    "after_drop = len(df_merged)\n",
    "dropped = before_drop - after_drop\n",
    "\n",
    "# Save the result\n",
    "df_merged.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"The data has been combined with the centrality indicators and saved to：{output_path}\")\n",
    "print(f\"Total: {after_drop} records, remove {dropped} records.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "#### Descriptive statistics (distribution maps, box plots, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/final/2015_2019_with_centrality.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "print(df.isnull().sum()) \n",
    "df.describe()\n",
    "df[\"accident_severity\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Distribution of accident severity\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=\"accident_severity\", data=df)\n",
    "plt.title(\"Accident Severity Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Numerical type: Number of vehicles, number of casualties, speed limit\n",
    "for col in [\"number_of_vehicles\", \"number_of_casualties\"]:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(np.log1p(df[col]), kde=True)\n",
    "    plt.title(f\"Log-transformed distribution of {col}\")\n",
    "    plt.xlabel(f\"log1p({col})\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(df[\"speed_limit\"], kde=True)\n",
    "plt.title(\"Distribution of Speed Limit\")\n",
    "plt.xlabel(\"speed_limit\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central variable distribution (single variable + null value check)\n",
    "for col in [\"mean_betweenness\", \"max_betweenness\", \"mean_degree\", \"max_degree\"]:\n",
    "    sns.histplot(df[col].dropna(), kde=True)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration of the Relationship between Features and Targets (including grouped bar charts and box plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The relationship between Variables and the severity of accidents (bivariate Analysis)\n",
    "for col in [\"mean_betweenness\", \"max_betweenness\", \"mean_degree\", \"max_degree\"]:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x=\"accident_severity\", y=col, data=df)\n",
    "    plt.title(f\"{col} vs Accident Severity\")\n",
    "    plt.xlabel(\"Accident Severity (1 = Fatal, 2 = Serious, 3 = Slight)\")\n",
    "    plt.ylabel(col)\n",
    "    plt.show()\n",
    "\n",
    "# Divide mean_betweenness into four grades (quartiles)\n",
    "df[\"betweenness_level\"] = pd.qcut(df[\"mean_betweenness\"], q=4, labels=[\"Low\", \"Medium-Low\", \"Medium-High\", \"High\"])\n",
    "\n",
    "# Check the proportion of accident severity in each group\n",
    "severity_by_level = pd.crosstab(df[\"betweenness_level\"], df[\"accident_severity\"], normalize='index')\n",
    "\n",
    "# Draw a grouped stacked bar chart\n",
    "severity_by_level.plot(kind=\"bar\", stacked=True, colormap=\"Set2\")\n",
    "plt.title(\"Accident Severity by Mean Betweenness Level\")\n",
    "plt.xlabel(\"Mean Betweenness Group\")\n",
    "plt.ylabel(\"Proportion of Accident Severity\")\n",
    "plt.legend(title=\"Severity\", loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "# Categorical variables can be analyzed in cross-tables:\n",
    "pd.crosstab(df[\"day_of_week\"], df[\"accident_severity\"], normalize='index').plot(kind='bar', stacked=True)\n",
    "plt.title(\"Accident Severity by Day of Week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the boxplots and grouped bar charts, maximum betweenness centrality (max_betweenness) shows stronger differentiation across accident severity levels, especially with higher values in fatal accidents. In contrast, mean_betweenness exhibits weaker variation, indicating a more subtle influence. Degree-based indicators, particularly max_degree, show very limited discriminative power and may not be useful in predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodological Flow Chart of Data Integration, Feature Engineering, Model Training, and Interpretation\n",
    "\n",
    "\n",
    "![image.png](../pic/flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Preparation (Feature Engineering)\n",
    "\n",
    "#### Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding + save\n",
    "categorical_vars = [\n",
    "    'day_of_week', 'road_type', 'light_conditions', 'weather_conditions',\n",
    "    'road_surface_conditions', 'junction_control', 'junction_detail',\n",
    "    'pedestrian_crossing_human_control', 'pedestrian_crossing_physical_facilities',\n",
    "    'special_conditions_at_site', 'first_road_class',\n",
    "    'second_road_class',\n",
    "    'trunk_road_flag', 'urban_or_rural_area'\n",
    "]\n",
    "\n",
    "# Coding\n",
    "df_encoded = pd.get_dummies(df.copy(), columns=categorical_vars, drop_first=True)\n",
    "\n",
    "# Convert the Boolean column to an integer\n",
    "for col in df_encoded.columns:\n",
    "    if df_encoded[col].dtype == 'bool':\n",
    "        df_encoded[col] = df_encoded[col].astype(int)\n",
    "\n",
    "# Check the distribution of data types\n",
    "print(\"Column types:\\n\", df_encoded.dtypes.value_counts())\n",
    "\n",
    "# get hour\n",
    "df_encoded[\"time_hour\"] = pd.to_datetime(df_encoded[\"time\"], format=\"%H:%M\", errors=\"coerce\").dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new variable time_hour was derived from the time field using datetime parsing, representing the hour of the accident. Records with missing or invalid time formats were excluded to ensure data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal encoding betweenness_level\n",
    "betweenness_mapping = {\n",
    "    'Low': 0,\n",
    "    'Medium-Low': 1,\n",
    "    'Medium-High': 2,\n",
    "    'High': 3\n",
    "}\n",
    "df_encoded['betweenness_level_encoded'] = df_encoded['betweenness_level'].map(betweenness_mapping)\n",
    "df_encoded.drop(columns=['betweenness_level'], inplace=True)\n",
    "\n",
    "# Delete the fields that cannot be modeled\n",
    "df_encoded.drop(columns=['time', 'borough', 'gss_code','local_authority_ons_district'], inplace=True)\n",
    "# Delete the post hoc variable\n",
    "df_encoded = df_encoded.drop(columns=['did_police_officer_attend_scene_of_accident', 'number_of_vehicles','number_of_casualties', 'carriageway_hazards'])\n",
    "print(df_encoded.columns)\n",
    "\n",
    "df_encoded.to_csv(\"../data/final/encode201519.csv\", index=False)\n",
    "print(\"Data saved to '../data/final/encode_all_years_with_centrality.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All categorical variables were either one-hot encoded or ordinal-encoded. The time variable was converted to time_hour, and betweenness_level was ordinally mapped to an integer scale. After removing non-modeling columns such as local_authority_ons_district, the final dataset included only numerical features and was free of missing values, making it ready for supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection & Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/final/encode201519.csv\")\n",
    "\n",
    "# View the basic structure\n",
    "print(\"DataFrame Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Missing value check\n",
    "print(\"\\nMissing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0].sort_values(ascending=False))\n",
    "\n",
    "# Data type statistics\n",
    "print(\"\\nData type distribution:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Check the object type field\n",
    "print(\"\\nObject Type fields and the number of their unique values:\")\n",
    "obj_cols = df.select_dtypes(include='object')\n",
    "print(obj_cols.nunique().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for modeling consists of 115,805 records with 97 numeric features after preprocessing. To simulate real-world forecasting, a temporal train-test split was adopted. Accidents from 2015–2018 were used for training, while 2019 data served as the hold-out test set. This temporal split ensures that the evaluation reflects the model's ability to generalize to future, unseen cases, rather than relying on random shuffling which may result in data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct features and labels\n",
    "X = df.drop(columns=[\"accident_severity\", \"accident_year\"])\n",
    "y = df[\"accident_severity\"]\n",
    "\n",
    "# Divide the training set and the test set by year\n",
    "X_train = X[df[\"accident_year\"].isin([2015, 2016, 2017, 2018])]\n",
    "X_test = X[df[\"accident_year\"] == 2019]\n",
    "y_train = y[df[\"accident_year\"].isin([2015, 2016, 2017, 2018])]\n",
    "y_test = y[df[\"accident_year\"] == 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, name=\"Model\"):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n🔍 {name} Classification Report\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Evaluation\n",
    "\n",
    "All three models were optimized using grid search with 3-fold cross-validation on the training set, based on macro-averaged F1-score as the evaluation metric. Macro-F1 is particularly appropriate for imbalanced multi-class classification problems, as it gives equal weight to each class regardless of sample size. This choice of scoring metric ensures that the models are not disproportionately tuned to the majority class performance, but instead maintain balanced treatment across all severity levels.\n",
    "\n",
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pipeline (Standardization + Logistic Regression)\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(max_iter=5000, random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter grid\n",
    "logreg_param_grid = {\n",
    "    'logreg__C': [0.01, 0.1, 1, 10],\n",
    "    'logreg__class_weight': ['balanced', None],\n",
    "    'logreg__multi_class': ['multinomial'],\n",
    "    'logreg__solver': ['lbfgs']\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid_search_logreg = GridSearchCV(\n",
    "    logreg_pipeline,\n",
    "    logreg_param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train\n",
    "grid_search_logreg.fit(X_train, y_train)\n",
    "print(\"Logistic Regression Optimal parameters:\", grid_search_logreg.best_params_)\n",
    "print(\"Logistic Regression The best macro-F1 score:\", grid_search_logreg.best_score_)\n",
    "\n",
    "# Prediction + Visualization\n",
    "y_pred_log = grid_search_logreg.best_estimator_.predict(X_test)\n",
    "print(\"\\nLogistic Regression Classification Report\")\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_log = confusion_matrix(y_test, y_pred_log)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_log, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Logistic Regression - Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('rf', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Parameter grid\n",
    "rf_param_grid = {\n",
    "    'rf__n_estimators': [100, 300],\n",
    "    'rf__max_depth': [10, 20, None],\n",
    "    'rf__min_samples_split': [2, 5],\n",
    "    'rf__class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid_search_rf = GridSearchCV(\n",
    "    rf_pipeline,\n",
    "    rf_param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Output result\n",
    "print(\"RF Optimal parameters:\", grid_search_rf.best_params_)\n",
    "print(\"RF The best macro-F1 score:\", grid_search_rf.best_score_)\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_rf = grid_search_rf.best_estimator_.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Reds')\n",
    "plt.title(\"Random Forest - Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create XGBoost pipeline\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('xgb', XGBClassifier(objective='multi:softprob', eval_metric='mlogloss', random_state=42, use_label_encoder=False))\n",
    "])\n",
    "\n",
    "# Parameter grid\n",
    "xgb_param_grid = {\n",
    "    'xgb__n_estimators': [100, 200],\n",
    "    'xgb__max_depth': [6, 10],\n",
    "    'xgb__learning_rate': [0.05, 0.1],\n",
    "    'xgb__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    xgb_pipeline,\n",
    "    xgb_param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "y_train = y_train - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "# Train\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# outcome\n",
    "print(\"✅ XGB Optimal parameters:\", grid_search_xgb.best_params_)\n",
    "print(\"✅ XGB The best macro-F1 score:\", grid_search_xgb.best_score_)\n",
    "\n",
    "# Prediction + Visualization\n",
    "y_pred_xgb = grid_search_xgb.best_estimator_.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"XGBoost - Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison and Final Selection\n",
    "\n",
    "Three supervised learning models were implemented to classify the severity of road traffic accidents: Logistic Regression, Random Forest, and XGBoost. Each model was evaluated based on its ability to capture class imbalance and distinguish between fatal, serious, and slight outcomes.\n",
    "\n",
    "Logistic Regression achieved high overall accuracy (0.88) but failed to correctly identify any fatal or serious cases, leading to a low macro-F1 score and no practical utility in real-world accident prevention.\n",
    "\n",
    "XGBoost offered improved performance over Logistic Regression in terms of macro-F1 and recall for the serious class, but remained heavily biased toward the majority class.\n",
    "\n",
    "Random Forest delivered the best overall balance between interpretability and performance, with a macro-F1 score of 0.35 and noticeably higher recall on minority classes. It also demonstrated stable results during cross-validation and allowed post-hoc interpretation using SHAP.\n",
    "\n",
    "\n",
    "#### Table: Comparison of Model Performance\n",
    "| Model               | Accuracy | Macro F1 | Precision (avg) | Recall (avg) | F1-score (avg) | Notable Issues                           |\n",
    "|--------------------|----------|----------|------------------|---------------|----------------|------------------------------------------|\n",
    "| Logistic Regression | 0.88     | 0.31     | 0.81             | 0.65          | 0.72           | Completely failed to detect fatal/serious |\n",
    "| Random Forest       | 0.81     | 0.35     | 0.75             | 0.68          | 0.72           | Most balanced, interpretable              |\n",
    "| XGBoost             | 0.85     | 0.32     | 0.79             | 0.66          | 0.72           | Still biased toward majority class        |\n",
    "\n",
    "\n",
    "\n",
    "Given these results, Random Forest was selected as the final model for its superior trade-off between classification performance and interpretability. Its output was further analysed using SHAP values, revealing that temporal and spatial network features were among the most influential predictors of accident severity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model interpretation\n",
    "#### Grouped Feature Importances (based on RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the RF part of the best model from the trained GridSearch\n",
    "rf_model = grid_search_rf.best_estimator_.named_steps['rf']\n",
    "\n",
    "# Use the column names of the training set as feature names\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Obtain the importance of features\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Group Aggregation importance\n",
    "grouped_importance = defaultdict(float)\n",
    "\n",
    "for feat, imp in zip(feature_names, importances):\n",
    "    match = re.match(r\"(.+?)_(\\d+)$\", feat)\n",
    "    if match:\n",
    "        base_feat = match.group(1)\n",
    "    else:\n",
    "        base_feat = feat\n",
    "    grouped_importance[base_feat] += imp\n",
    "\n",
    "# trans to DataFrame\n",
    "grouped_df = pd.DataFrame({\n",
    "    'Feature Group': list(grouped_importance.keys()),\n",
    "    'Total Importance': list(grouped_importance.values())\n",
    "}).sort_values(by='Total Importance', ascending=False)\n",
    "\n",
    "# visualize top 10\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(grouped_df['Feature Group'][:10][::-1], grouped_df['Total Importance'][:10][::-1])\n",
    "plt.xlabel(\"Aggregated Importance\")\n",
    "plt.title(\"Top 10 Grouped Feature Importances (Random Forest)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# export\n",
    "print(grouped_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "best_pipeline_rf = grid_search_rf.best_estimator_\n",
    "rf_model = best_pipeline_rf.named_steps['rf']\n",
    "X_train_raw = X_train.copy()\n",
    "explainer = shap.Explainer(rf_model, X_train_raw)\n",
    "\n",
    "# This step will cost about 45 min\n",
    "shap_values = explainer(X_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mean_abs_shap = np.abs(shap_values.values).mean(axis=(0, 2))  # shape: (85,)\n",
    "\n",
    "feature_names = X_train_raw.columns\n",
    "assert len(mean_abs_shap) == len(feature_names), \"Mismatch between SHAP values and feature names\"\n",
    "\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'mean_abs_shap': mean_abs_shap\n",
    "})\n",
    "\n",
    "def get_base_feature(f):\n",
    "    parts = f.split('_')\n",
    "    if parts[-1].isdigit() and len(parts) > 2:\n",
    "        return '_'.join(parts[:-1])\n",
    "    elif parts[-1].isdigit():\n",
    "        return parts[0]\n",
    "    return f\n",
    "\n",
    "shap_df['base_feature'] = shap_df['feature'].apply(get_base_feature)\n",
    "\n",
    "grouped_shap = shap_df.groupby('base_feature')['mean_abs_shap'].sum().reset_index()\n",
    "grouped_shap = grouped_shap.sort_values(by='mean_abs_shap', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(grouped_shap['base_feature'], grouped_shap['mean_abs_shap'])\n",
    "plt.xlabel('Mean(|SHAP value|)')\n",
    "plt.title('Grouped SHAP Importance (by Feature Group)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mean_abs_shap = np.abs(shap_values.values).mean(axis=(0, 2))  # shape: (85,)\n",
    "\n",
    "feature_names = X_train_raw.columns\n",
    "assert len(mean_abs_shap) == len(feature_names), \"Mismatch between SHAP values and feature names\"\n",
    "\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'mean_abs_shap': mean_abs_shap\n",
    "})\n",
    "\n",
    "def get_base_feature(f):\n",
    "    parts = f.split('_')\n",
    "    if parts[-1].isdigit() and len(parts) > 2:\n",
    "        return '_'.join(parts[:-1])\n",
    "    elif parts[-1].isdigit():\n",
    "        return parts[0]\n",
    "    return f\n",
    "\n",
    "shap_df['base_feature'] = shap_df['feature'].apply(get_base_feature)\n",
    "\n",
    "grouped_shap = shap_df.groupby('base_feature')['mean_abs_shap'].sum().reset_index()\n",
    "grouped_shap = grouped_shap.sort_values(by='mean_abs_shap', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(grouped_shap['base_feature'], grouped_shap['mean_abs_shap'])\n",
    "plt.xlabel('Mean(|SHAP value|)')\n",
    "plt.title('Grouped SHAP Importance (by Feature Group)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train_raw.columns\n",
    "\n",
    "class_names = ['Fatal', 'Serious', 'Slight']\n",
    "shap_grouped_by_class = {cls: defaultdict(float) for cls in class_names}\n",
    "\n",
    "for class_idx, class_label in enumerate(class_names):\n",
    "    shap_vals = shap_values.values[:, :, class_idx]\n",
    "    shap_mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "\n",
    "    for feat_name, shap_val in zip(feature_names, shap_mean_abs):\n",
    "        match = re.match(r\"(.+?)_(\\d+)$\", feat_name)\n",
    "        base_feat = match.group(1) if match else feat_name\n",
    "        shap_grouped_by_class[class_label][base_feat] += shap_val\n",
    "\n",
    "all_features = sorted(set().union(*[d.keys() for d in shap_grouped_by_class.values()]))\n",
    "df_plot = pd.DataFrame({\n",
    "    'Feature Group': all_features,\n",
    "    'Fatal': [shap_grouped_by_class['Fatal'].get(f, 0) for f in all_features],\n",
    "    'Serious': [shap_grouped_by_class['Serious'].get(f, 0) for f in all_features],\n",
    "    'Slight': [shap_grouped_by_class['Slight'].get(f, 0) for f in all_features]\n",
    "})\n",
    "\n",
    "df_plot['Total'] = df_plot['Fatal'] + df_plot['Serious'] + df_plot['Slight']\n",
    "df_top10 = df_plot.sort_values(by='Total', ascending=False).head(10)\n",
    "\n",
    "x = np.arange(len(df_top10['Feature Group']))\n",
    "width = 0.25\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width, df_top10['Fatal'], width, label='Fatal', color='royalblue')\n",
    "plt.bar(x, df_top10['Serious'], width, label='Serious', color='deeppink')\n",
    "plt.bar(x + width, df_top10['Slight'], width, label='Slight', color='olive')\n",
    "\n",
    "plt.xticks(x, df_top10['Feature Group'], rotation=45, ha='right')\n",
    "plt.ylabel('Mean(|SHAP value|)')\n",
    "plt.title('Top 10 Grouped SHAP Importances by Class')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP (SHapley Additive exPlanations) was employed to interpret model predictions and evaluate feature importance at both global and class-specific levels. This method is grounded in cooperative game theory and attributes model output to each input feature fairly and consistently. The global SHAP summary plots reveal the average magnitude of feature contributions across all predictions, whereas the class-wise SHAP decomposition further isolates the feature influence on fatal, serious, and slight injuries separately. This multi-layered interpretation enhances model transparency and facilitates policy-relevant insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and discussion\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three supervised learning models were trained to classify accident severity: Logistic Regression, Random Forest, and XGBoost, using a dataset of **115,805 records** from 2015–2019 with **97 numerical features**. The target variable had a highly imbalanced distribution: **“slight” = 87.6%**, **“serious” = 11.9%**, and **“fatal” = 0.5%**, making **macro-F1 and per-class recall** more appropriate than accuracy for evaluation.\n",
    "\n",
    "- **Logistic Regression** achieved the highest overall accuracy (**0.88**), but failed to detect any “fatal” or “serious” cases (macro-F1 = **0.31**).  \n",
    "- **XGBoost** offered slightly better recall for the “serious” class and achieved a macro-F1 of **0.32**, but remained heavily biased toward the majority class.  \n",
    "- **Random Forest** delivered the most balanced performance, with an accuracy of **0.81** and a macro-F1 score of **0.35**, successfully identifying a subset of minority cases (recall: fatal = 0.02, serious = 0.08).\n",
    "\n",
    "These results illustrate the risk of relying on accuracy in imbalanced classification. For example, **Kumar and Teja Santosh (2022)** reported 96.18% accuracy using XGBoost, but did not address class imbalance or report recall, making such models less reliable in safety-critical applications. This study emphasizes metrics that reflect model fairness across all classes.\n",
    "\n",
    "Random Forest’s feature importance revealed that `time_hour`, `day_of_week`, and `max_betweenness` were consistently impactful. `Max_betweenness`, a spatial indicator derived from borough-level road network topology, appeared among the top 5 predictors, validating the integration of spatial structure into severity modeling.\n",
    "\n",
    "To further interpret the model, **SHAP (SHapley Additive Explanations)** was applied. Global SHAP analysis confirmed the high contribution of temporal and spatial features. Class-specific SHAP bar plots showed that:\n",
    "- For **fatal** accidents, the most influential features were `max_betweenness`, `speed_limit`, and `light_conditions_5` (dark, no street lighting).\n",
    "- For **serious** accidents, `junction_detail`, `pedestrian_crossing_physical_facilities`, and `first_road_class` played larger roles.\n",
    "- For **slight** accidents, `time_hour`, `weather_conditions`, and general road context were dominant.\n",
    "\n",
    "These findings demonstrate that combining **spatial network metrics** with **contextual accident features** enhances both predictive accuracy and model interpretability for road safety applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This study investigated whether supervised machine learning models can accurately predict the severity of road traffic accidents in London, using spatial, temporal, and environmental features. A borough-level dataset from 2015 to 2019 was constructed, integrating UK accident records with spatial centrality indicators derived from OpenStreetMap.\n",
    "\n",
    "Among the models tested, Random Forest demonstrated the best overall balance between performance and interpretability, achieving a macro-F1 score of 0.35. SHAP analysis further revealed that features such as `max_betweenness`, `time_hour`, and `road type` significantly contributed to severity predictions. These results suggest that incorporating spatial network metrics meaningfully enhances the capacity of data-driven safety models to identify severe accident risks.\n",
    "\n",
    "However, this project has several limitations. The severe class imbalance in the dataset limited the model’s ability to generalize predictions for rare fatal cases. The spatial resolution was restricted to the borough level, which may obscure finer-scale local effects. In addition, the study only used tabular features; incorporating trajectory-level or vehicle-specific data could improve model fidelity.\n",
    "\n",
    "Future work may explore finer spatial units, additional data modalities (e.g., street view imagery or traffic flow), and deep learning methods for enhanced prediction and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abdel-Aty, M. and Haleem, K. (2011) ‘Analyzing angle crashes at unsignalized intersections using machine learning techniques’, Accident Analysis & Prevention, 43(1), pp. 461–470. Available at: https://doi.org/10.1016/j.aap.2010.10.002.\n",
    "\n",
    "Ahmed, S. et al. (2023) ‘A study on road accident prediction and contributing factors using explainable machine learning models: analysis and performance’, Transportation Research Interdisciplinary Perspectives, 19, p. 100814. Available at: https://doi.org/10.1016/j.trip.2023.100814.\n",
    "\n",
    "Kumar, A.P. and Teja Santosh, D. (2022) ‘Road Accident Severity Prediction Using Machine Learning Algorithms’, International Journal of Computer Engineering in Research Trends, 9(9), pp. 175–183. Available at: https://doi.org/10.22362/ijcert/2022/v9/i9/v9i902.\n",
    "\n",
    "Department for Transport (DfT), 2013. New penalties for careless driving come into force. [online] Available at: https://www.gov.uk/government/news/new-penalties-for-careless-driving-come-into-force [Accessed 20 Apr. 2025].\n",
    "\n",
    "Department for Transport (DfT), 2021. Impacts of 2020 Low Traffic Neighbourhoods in London on Road Traffic Injuries. [pdf] Available at: https://assets.publishing.service.gov.uk/media/65f400adfa18510011011787/low-traffic-neighbourhoods-research-report.pdf [Accessed 20 Apr. 2025].\n",
    "\n",
    "Transport for London (TfL), 2024. The impacts of Low Traffic Neighbourhoods in London. [pdf] Available at: https://tfl.gov.uk/cdn/static/cms/documents/tfl-impacts-of-low-traffic-neighbourhoods-feb-2024-acc.pdf [Accessed 20 Apr. 2025]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
